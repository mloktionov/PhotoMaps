{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2272643-aefa-49b2-9027-e780e0288c82",
   "metadata": {},
   "source": [
    "# Pipeline:\n",
    "\n",
    "1. Check stats for images folder and included albums\n",
    "2. Run MAIN script. It will create photos/ folder with photos, encoded from different formats from images/ folder. It will also create photos.csv with census of files and missing_coords.csv with the ones which do not have any coords. Source files (jpg, jpeg, heic) are transformed into jpg files according to the template IMG_YYYYMMDD_has.jpg\n",
    "3. Run a script to pick up Description field from Google Photos. It uses Album+source_filename to identify photo. It records the results into photos_description.csv\n",
    "4. Add coordinates to missing_coords.csv manually\n",
    "5. Run the next script which will merge photos.csv+missing_coords.csv+photos_descriptions.csv into photos_cobmined.csv which contains the full list of all photos with their names, coordinates, dates and description. \n",
    "6. Upload files from photos/ to Google Drive/photos\n",
    "7. Connect to Google Drive and get new filenames with their links into google_drive_links.csv\n",
    "8. Get all info from photos_combined.csv and google_drive_links.csv into photos_final.csv\n",
    "9. Put all info from photos_final.csv into photos.geojson\n",
    "10. Get country code from nominate site into photos.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193bd5-4063-4e45-905e-5e7f3b046d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825b12a-386c-4546-963e-f90500281dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- STEP 1. Check stats summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c4119-d6a1-4ccd-9849-cb6541bd0524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# === –ü–∞–ø–∫–∞ —Å –∞–ª—å–±–æ–º–∞–º–∏ ===\n",
    "ROOT_DIR = \"images\"\n",
    "album_dirs = [d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "\n",
    "# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á—ë—Ç—á–∏–∫–æ–≤ ===\n",
    "stats = {}\n",
    "total = defaultdict(int)\n",
    "\n",
    "# === –®–∞–±–ª–æ–Ω—ã ===\n",
    "pattern_copy = re.compile(r\"\\s\\(\\d+\\)| copy\", re.IGNORECASE)\n",
    "\n",
    "# === –ê–Ω–∞–ª–∏–∑ –ø–æ –∞–ª—å–±–æ–º–∞–º ===\n",
    "for album in album_dirs:\n",
    "    folder_path = os.path.join(ROOT_DIR, album)\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "\n",
    "        if ext in ['.jpg', '.jpeg', '.heic']:\n",
    "            if '-edited' in file.lower():\n",
    "                counts['edited'] += 1\n",
    "            else:\n",
    "                counts[ext] += 1\n",
    "                if pattern_copy.search(file):\n",
    "                    counts['renamed'] += 1\n",
    "        elif ext == '.json':\n",
    "            counts['json'] += 1\n",
    "        else:\n",
    "            counts['other'] += 1\n",
    "\n",
    "    stats[album] = dict(counts)\n",
    "    for k, v in counts.items():\n",
    "        total[k] += v\n",
    "\n",
    "# === –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ===\n",
    "print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–ª—å–±–æ–º–∞–º:\")\n",
    "for album, counts in stats.items():\n",
    "    print(f\"\\nüìÅ {album}:\")\n",
    "    for k, v in counts.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# === –ü–æ–¥—Å—á—ë—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–≥–æ–¥–Ω—ã—Ö —Ñ–æ—Ç–æ ===\n",
    "photo_exts = ['.jpg', '.jpeg', '.heic']\n",
    "total_photos = sum(total[ext] for ext in photo_exts)\n",
    "total_edited = total['edited']\n",
    "usable_photos = total_photos  # editable —É–∂–µ –∏—Å–∫–ª—é—á–µ–Ω—ã –≤—ã—à–µ, –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è\n",
    "\n",
    "print(\"\\nüìà –ò—Ç–æ–≥–æ –ø–æ –≤—Å–µ–º –∞–ª—å–±–æ–º–∞–º:\")\n",
    "for k, v in total.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nüßÆ –í—Å–µ–≥–æ —Ñ–æ—Ç–æ-—Ñ–∞–π–ª–æ–≤ (–±–µ–∑ -edited): {total_photos}\")\n",
    "print(f\"‚úÇÔ∏è  –§–∞–π–ª–æ–≤ —Å '-edited' –≤ –∏–º–µ–Ω–∏:        {total_edited}\")\n",
    "print(f\"‚úÖ –ì–æ–¥–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤:        {usable_photos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16c5c9-12e9-47b7-ac86-4ecf05583819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2. MAIN script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c580532-e4ba-44f9-84d3-77270965d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 1427/2431 [06:35<05:51,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.0842092, lon: 14.4240219, file: IMG_1894.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_1894.HEIC ‚Üí IMG_20220501_332311.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1546/2431 [07:11<03:42,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.082902600000004, lon: 14.422433299999998, file: IMG_1899.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_1899.HEIC ‚Üí IMG_20220501_71799.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1694/2431 [07:56<03:47,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.0877207, lon: 14.4277908, file: IMG_1811.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_1811.HEIC ‚Üí IMG_20220501_135512.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1790/2431 [08:25<02:58,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.0864771, lon: 14.411436600000002, file: IMG_2005.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_2005.HEIC ‚Üí IMG_20220501_551842.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1810/2431 [08:31<03:08,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.075538099999996, lon: 14.437800500000002, file: IMG_1890.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_1890.HEIC ‚Üí IMG_20220501_429680.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1846/2431 [08:41<02:56,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: 50.0861017, lon: 14.416173599999997, file: IMG_1901.HEIC\n",
      "[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: IMG_1901.HEIC ‚Üí IMG_20220501_435404.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [10:46<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä –°–≤–æ–¥–∫–∞:\n",
      "üßÆ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 2431\n",
      "‚úÖ –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏:   2424\n",
      "‚ùå –ë–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç:    7\n",
      "üìÑ CSV-—Ñ–∞–π–ª—ã:        /Users/mloktionov/PycharmProjects/PhotoMaps/csv/photos.csv, /Users/mloktionov/PycharmProjects/PhotoMaps/csv/missing_coords.csv\n",
      "üìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:      /Users/mloktionov/PycharmProjects/PhotoMaps/photos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from PIL import Image, ExifTags\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Paths ===\n",
    "ROOT = os.path.abspath(\".\")\n",
    "SRC_FOLDER = os.path.join(ROOT, \"images\")\n",
    "OUT_FOLDER = os.path.join(ROOT, \"photos\")\n",
    "CSV_FOLDER = os.path.join(ROOT, \"csv\")\n",
    "CSV_MAIN = os.path.join(CSV_FOLDER, \"photos.csv\")\n",
    "CSV_MISSING = os.path.join(CSV_FOLDER, \"missing_coords.csv\")\n",
    "\n",
    "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Helpers ===\n",
    "def convert_to_degrees(v):\n",
    "    d, m, s = v\n",
    "    return float(d) + float(m) / 60 + float(s) / 3600\n",
    "\n",
    "def extract_gps_from_exif(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif()\n",
    "        if not exif_data:\n",
    "            return None, None\n",
    "\n",
    "        gps_info = {}\n",
    "        date_str = None\n",
    "        for tag, value in exif_data.items():\n",
    "            decoded = ExifTags.TAGS.get(tag)\n",
    "            if decoded == \"GPSInfo\":\n",
    "                for t in value:\n",
    "                    sub_decoded = ExifTags.GPSTAGS.get(t)\n",
    "                    gps_info[sub_decoded] = value[t]\n",
    "            elif decoded == \"DateTimeOriginal\":\n",
    "                date_str = value\n",
    "\n",
    "        if not gps_info:\n",
    "            return None, date_str\n",
    "\n",
    "        lat = convert_to_degrees(gps_info.get(\"GPSLatitude\"))\n",
    "        if gps_info.get(\"GPSLatitudeRef\") == \"S\":\n",
    "            lat = -lat\n",
    "        lon = convert_to_degrees(gps_info.get(\"GPSLongitude\"))\n",
    "        if gps_info.get(\"GPSLongitudeRef\") == \"W\":\n",
    "            lon = -lon\n",
    "\n",
    "        return (lat, lon), date_str\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def extract_gps_from_json(json_path):\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            geo = data.get(\"geoData\")\n",
    "            photo_taken_time = data.get(\"photoTakenTime\", {}).get(\"timestamp\")\n",
    "            if geo:\n",
    "                lat = geo.get(\"latitude\")\n",
    "                lon = geo.get(\"longitude\")\n",
    "                return (lat, lon), photo_taken_time\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def find_json_path_for_image(image_path):\n",
    "    base = os.path.basename(image_path)\n",
    "    folder = os.path.dirname(image_path)\n",
    "\n",
    "    suffixes = [\n",
    "        \".supplemental-metadata.json\",\n",
    "        \".supplemental-meta.json\",\n",
    "        \".supplemental-metada.json\",\n",
    "        \".supplemental-metadat.json\",\n",
    "        \".supplemental-me.json\"\n",
    "    ]\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        candidate = image_path + suffix\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "\n",
    "    match = re.match(r'^(.*)\\((\\d+)\\)\\.(jpg|jpeg|heic)$', base, re.IGNORECASE)\n",
    "    if match:\n",
    "        base_clean = match.group(1).strip()\n",
    "        suffix_num = match.group(2)\n",
    "        ext = match.group(3)\n",
    "        for sfx in suffixes:\n",
    "            sfx_with_index = sfx.replace(\".json\", f\"({suffix_num}).json\")\n",
    "            alt_json = os.path.join(folder, f\"{base_clean}.{ext}{sfx_with_index}\")\n",
    "            if os.path.exists(alt_json):\n",
    "                return alt_json\n",
    "\n",
    "    return None\n",
    "\n",
    "def parse_date_components(date_str):\n",
    "    try:\n",
    "        if date_str and len(str(date_str)) == 10 and str(date_str).isdigit():\n",
    "            dt = datetime.fromtimestamp(int(date_str))\n",
    "        else:\n",
    "            dt = datetime.strptime(date_str, \"%Y:%m:%d %H:%M:%S\")\n",
    "        return str(dt.year), f\"{dt.month:02d}\", f\"{dt.day:02d}\"\n",
    "    except:\n",
    "        return \"2099\", \"01\", \"01\"\n",
    "\n",
    "def spiral_coords(lat, lon, index, step=0.0002):\n",
    "    angle = index * (math.pi / 3)\n",
    "    radius = step * (1 + index // 6)\n",
    "    return lat + radius * math.cos(angle), lon + radius * math.sin(angle)\n",
    "\n",
    "# === Find all images ===\n",
    "image_files = []\n",
    "for dirpath, _, filenames in os.walk(SRC_FOLDER):\n",
    "    for f in filenames:\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.heic')) and \"-edited\" not in f.lower():\n",
    "            image_files.append(os.path.join(dirpath, f))\n",
    "\n",
    "# === Processing ===\n",
    "Image.init()\n",
    "coords_seen = {}\n",
    "records_ok = []\n",
    "records_missing = []\n",
    "\n",
    "for in_path in tqdm(image_files, desc=\"\\U0001f4e6 –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\", ncols=80):\n",
    "    source_filename = os.path.basename(in_path)\n",
    "    ext = os.path.splitext(source_filename)[1].lower()\n",
    "    album = os.path.basename(os.path.dirname(in_path))\n",
    "    source_type = None\n",
    "    lat, lon, date_str = None, None, None\n",
    "\n",
    "    try:\n",
    "        if ext in [\".jpg\", \".jpeg\"]:\n",
    "            gps, date_str = extract_gps_from_exif(in_path)\n",
    "            if gps:\n",
    "                lat, lon = gps\n",
    "                source_type = \"exif\"\n",
    "\n",
    "        if lat is None or lon is None:\n",
    "            json_path = find_json_path_for_image(in_path)\n",
    "            if json_path:\n",
    "                gps_json, json_date = extract_gps_from_json(json_path)\n",
    "                if gps_json is not None:\n",
    "                    try:\n",
    "                        lat = float(gps_json[0])\n",
    "                        lon = float(gps_json[1])\n",
    "                        source_type = (source_type or \"\") + \"+json\"\n",
    "                        if ext == \".heic\":\n",
    "                            print(f\"[HEIC] –ù–∞–π–¥–µ–Ω JSON ‚Üí lat: {lat}, lon: {lon}, file: {source_filename}\")\n",
    "                    except:\n",
    "                        lat, lon = None, None\n",
    "                if not date_str and json_date:\n",
    "                    date_str = json_date\n",
    "\n",
    "        year, month, day = parse_date_components(date_str)\n",
    "        base_name = f\"IMG_{year}{month}{day}_{abs(hash(source_filename)) % 10**6}.jpg\"\n",
    "        out_path = os.path.join(OUT_FOLDER, base_name)\n",
    "\n",
    "        if ext == \".heic\":\n",
    "            print(f\"[HEIC] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: {source_filename} ‚Üí {base_name}\")\n",
    "            temp_path = \"/tmp/temp_output.jpg\"\n",
    "            result = subprocess.run([\n",
    "                \"sips\", \"-s\", \"format\", \"jpeg\", in_path, \"--out\", temp_path\n",
    "            ], capture_output=True, text=True)\n",
    "\n",
    "            if result.returncode == 0 and os.path.exists(temp_path):\n",
    "                image = Image.open(temp_path)\n",
    "                width, height = image.size\n",
    "                new_size = (int(width * 0.8), int(height * 0.8))\n",
    "                image = image.resize(new_size, Image.LANCZOS)\n",
    "                image.convert(\"RGB\").save(out_path, \"JPEG\", quality=85)\n",
    "                os.remove(temp_path)\n",
    "            else:\n",
    "                print(f\"[HEIC] ‚ùå –û—à–∏–±–∫–∞ sips: {result.stderr.strip()}\")\n",
    "                Image.new(\"RGB\", (800, 600), (128, 128, 128)).save(out_path, \"JPEG\", quality=85)\n",
    "                source_type = (source_type or \"\") + \"+heic-error\"\n",
    "\n",
    "        else:\n",
    "            image = Image.open(in_path)\n",
    "            width, height = image.size\n",
    "            new_size = (int(width * 0.8), int(height * 0.8))\n",
    "            image = image.resize(new_size, Image.LANCZOS)\n",
    "            image.convert(\"RGB\").save(out_path, \"JPEG\", quality=85)\n",
    "\n",
    "        row = {\n",
    "            'filename': base_name,\n",
    "            'folder': os.path.basename(OUT_FOLDER),\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'album': album,\n",
    "            'source_filename': source_filename,\n",
    "            'source_type': source_type\n",
    "        }\n",
    "\n",
    "        if lat in (None, 0.0) or lon in (None, 0.0):\n",
    "            row['latitude'] = None\n",
    "            row['longitude'] = None\n",
    "            records_missing.append(row)\n",
    "        else:\n",
    "            key = (round(lat, 6), round(lon, 6))\n",
    "            count = coords_seen.get(key, 0)\n",
    "            if count > 0:\n",
    "                row['latitude'], row['longitude'] = spiral_coords(lat, lon, count)\n",
    "            coords_seen[key] = count + 1\n",
    "            records_ok.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {source_filename}: {e}\")\n",
    "        year, month, day = \"2099\", \"01\", \"01\"\n",
    "        base_name = f\"IMG_{year}{month}{day}_{abs(hash(source_filename)) % 10**6}.jpg\"\n",
    "        records_missing.append({\n",
    "            'filename': base_name,\n",
    "            'folder': os.path.basename(OUT_FOLDER),\n",
    "            'latitude': None,\n",
    "            'longitude': None,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'album': album,\n",
    "            'source_filename': source_filename,\n",
    "            'source_type': None\n",
    "        })\n",
    "\n",
    "# === Save CSVs ===\n",
    "pd.DataFrame(records_ok).to_csv(CSV_MAIN, index=False)\n",
    "pd.DataFrame(records_missing).to_csv(CSV_MISSING, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n\\U0001f4ca –°–≤–æ–¥–∫–∞:\")\n",
    "print(f\"\\U0001f9ee –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(records_ok) + len(records_missing)}\")\n",
    "print(f\"‚úÖ –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏:   {len(records_ok)}\")\n",
    "print(f\"‚ùå –ë–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç:    {len(records_missing)}\")\n",
    "print(f\"üìÑ CSV-—Ñ–∞–π–ª—ã:        {CSV_MAIN}, {CSV_MISSING}\")\n",
    "print(f\"üìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:      {OUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459498b4-8fa0-4bae-9a83-749aca017359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739417a-be70-47df-871a-4018f81a7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 3. Getting descriptions from Google photos via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1499347c-a889-4274-b2d9-9188fc3bc7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=849244129200-hdfqohk1rs46hjekajgu7pa4jqrn9sqj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A60939%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fphotoslibrary.readonly&state=KCVP37DvJmxZEqfefrdyKQwSp3XioA&access_type=offline\n",
      "üîç –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ –≤ Google Photos...\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–ª—å–±–æ–º—ã: ['PhotoMap 2022-2025', 'PhotoMap 2019-2021', 'PhotoMap 2017-2019', 'PhotoMap 2014-2016', 'PhotoMap 2010-2013', 'PhotoMap 08-09']\n",
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2022-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2022-2025: 653—Ñ–æ—Ç–æ [00:06, 107.97—Ñ–æ—Ç–æ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2019-2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2019-2021: 784—Ñ–æ—Ç–æ [00:08, 96.14—Ñ–æ—Ç–æ/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2017-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2017-2019: 249—Ñ–æ—Ç–æ [00:02, 101.87—Ñ–æ—Ç–æ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2014-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2014-2016: 381—Ñ–æ—Ç–æ [00:03, 102.93—Ñ–æ—Ç–æ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2010-2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2010-2013: 257—Ñ–æ—Ç–æ [00:02, 111.37—Ñ–æ—Ç–æ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 08-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 08-09: 109—Ñ–æ—Ç–æ [00:01, 67.24—Ñ–æ—Ç–æ/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –û–ø–∏—Å–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ csv/photos_descriptions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build_from_document\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===\n",
    "SCOPES = ['https://www.googleapis.com/auth/photoslibrary.readonly']\n",
    "OUTPUT_CSV = 'csv/photos_descriptions.csv'\n",
    "ALBUM_NAMES = [\n",
    "    \"PhotoMap 2022-2025\", \"PhotoMap 2019-2021\", \"PhotoMap 2017-2019\",\n",
    "    \"PhotoMap 2014-2016\", \"PhotoMap 2010-2013\", \"PhotoMap 08-09\"\n",
    "]\n",
    "\n",
    "# === –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è ===\n",
    "flow = InstalledAppFlow.from_client_secrets_file('client_secrets.json', SCOPES)\n",
    "credentials = flow.run_local_server(port=0)\n",
    "\n",
    "# === –ß—Ç–µ–Ω–∏–µ discovery JSON ===\n",
    "with open(\"photoslibrary_v1_discovery.json\", \"r\") as f:\n",
    "    discovery_doc = f.read()\n",
    "\n",
    "service = build_from_document(discovery_doc, credentials=credentials)\n",
    "\n",
    "# === –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ ===\n",
    "print(\"üîç –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ –≤ Google Photos...\")\n",
    "albums = []\n",
    "nextPageToken = None\n",
    "\n",
    "while True:\n",
    "    response = service.albums().list(pageSize=50, pageToken=nextPageToken).execute()\n",
    "    albums.extend(response.get('albums', []))\n",
    "    nextPageToken = response.get('nextPageToken')\n",
    "    if not nextPageToken:\n",
    "        break\n",
    "\n",
    "target_albums = {album['title']: album['id'] for album in albums if album['title'] in ALBUM_NAMES}\n",
    "\n",
    "# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–ª—å–±–æ–º–æ–≤ ===\n",
    "if not target_albums:\n",
    "    print(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∞–ª—å–±–æ–º—ã –≤ Google Photos\")\n",
    "else:\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–ª—å–±–æ–º—ã: {list(target_albums.keys())}\")\n",
    "\n",
    "# === –ó–∞–ø–∏—Å—å –æ–ø–∏—Å–∞–Ω–∏–π ===\n",
    "os.makedirs('csv', exist_ok=True)\n",
    "with open(OUTPUT_CSV, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['album', 'source_filename', 'description'])\n",
    "\n",
    "    for album_name, album_id in target_albums.items():\n",
    "        print(f\"\\nüìÇ –ê–ª—å–±–æ–º: {album_name}\")\n",
    "        nextPageToken = None\n",
    "\n",
    "        with tqdm(desc=f\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {album_name}\", unit=\"—Ñ–æ—Ç–æ\", leave=True) as pbar:\n",
    "            while True:\n",
    "                results = service.mediaItems().search(\n",
    "                    body={\"albumId\": album_id, \"pageSize\": 100, \"pageToken\": nextPageToken}\n",
    "                ).execute()\n",
    "\n",
    "                items = results.get('mediaItems', [])\n",
    "                nextPageToken = results.get('nextPageToken')\n",
    "\n",
    "                if not items:\n",
    "                    break\n",
    "\n",
    "                for item in items:\n",
    "                    description = item.get('description')\n",
    "                    filename = item.get('filename')\n",
    "\n",
    "                    if description and filename:\n",
    "                        writer.writerow([album_name, filename, description])\n",
    "\n",
    "                pbar.update(len(items))\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "\n",
    "    print(f\"\\n‚úÖ –û–ø–∏—Å–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cfeb797-eb24-4cc8-a9b8-bdf3535a31e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- STEP 4. here add coordinates to missing_coords.csv\n",
    "# --- and then run the script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55a8b8-5696-4551-a911-3e59893808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5. Run the script to merge all csv into one containing file names, coords, dates and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f05ece5e-8b01-436d-88ca-3da27b0cb07a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV...\n",
      "\n",
      "‚úÖ –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: csv/photos_combined.csv\n",
      "üìÑ –°—Ç—Ä–æ–∫:  2431\n",
      "üìù –ö–æ–ª–æ–Ω–æ–∫:  ['filename', 'folder', 'latitude', 'longitude', 'year', 'month', 'day', 'album', 'source_filename', 'source_type', 'description']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === –ü—É—Ç–∏ ===\n",
    "CSV_FOLDER = \"csv\"\n",
    "MAIN_CSV = os.path.join(CSV_FOLDER, \"photos.csv\")\n",
    "MISSING_CSV = os.path.join(CSV_FOLDER, \"missing_coords.csv\")\n",
    "DESCRIPTIONS_CSV = os.path.join(CSV_FOLDER, \"photos_descriptions.csv\")\n",
    "FINAL_CSV = os.path.join(CSV_FOLDER, \"photos_combined.csv\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ ===\n",
    "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV...\")\n",
    "df_main = pd.read_csv(MAIN_CSV)\n",
    "df_missing = pd.read_csv(MISSING_CSV)\n",
    "df_desc = pd.read_csv(DESCRIPTIONS_CSV)\n",
    "\n",
    "# === –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ main –∏ missing ===\n",
    "df_all = pd.concat([df_main, df_missing], ignore_index=True)\n",
    "\n",
    "# === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è merge –ø–æ –∫–ª—é—á—É: album + source_filename ===\n",
    "df_all[\"merge_key\"] = df_all[\"album\"].str.strip() + \"/\" + df_all[\"source_filename\"].str.strip()\n",
    "df_desc[\"merge_key\"] = df_desc[\"album\"].str.strip() + \"/\" + df_desc[\"source_filename\"].str.strip()\n",
    "\n",
    "# === –°–ª–∏—è–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π ===\n",
    "df_all = df_all.merge(\n",
    "    df_desc[[\"merge_key\", \"description\"]],\n",
    "    on=\"merge_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å ===\n",
    "df_all.drop(columns=[\"merge_key\"], inplace=True)\n",
    "df_all.to_csv(FINAL_CSV, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω:\", FINAL_CSV)\n",
    "print(\"üìÑ –°—Ç—Ä–æ–∫: \", len(df_all))\n",
    "print(\"üìù –ö–æ–ª–æ–Ω–æ–∫: \", list(df_all.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce6609-170d-4f72-bf81-5290f35923a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671d963-26b5-43b3-953b-90009d2edf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 6. Upload files from photos/ to Google Drive/photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc1988-6bd3-4710-93cb-837108d5c9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Script deletes all files in Google Drive/Photos\n",
    "# --- Script uploads all files from photos/ to Google/Photos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6fc98cd-fa3b-49d7-a2d9-93ffb9c3c475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=849244129200-hdfqohk1rs46hjekajgu7pa4jqrn9sqj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n",
      "üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [23:06<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–∞–ø–∫–∞ –æ—á–∏—â–µ–Ω–∞.\n",
      "‚¨ÜÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [2:28:50<00:00,  3.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# === –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# === ID —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–∏ –Ω–∞ Google Drive\n",
    "FOLDER_ID = '1em81MElkxnaue5r92e9hPGezgOuw4nRL'\n",
    "LOCAL_PHOTOS_FOLDER = 'photos/'\n",
    "\n",
    "# === –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive\n",
    "print(\"üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive...\")\n",
    "file_list = drive.ListFile({\n",
    "    'q': f\"'{FOLDER_ID}' in parents and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for f in tqdm(file_list, desc=\"–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\"):\n",
    "    f.Delete()\n",
    "\n",
    "print(\"‚úÖ –ü–∞–ø–∫–∞ –æ—á–∏—â–µ–Ω–∞.\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏\n",
    "print(\"‚¨ÜÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ Google Drive...\")\n",
    "local_files = [f for f in os.listdir(LOCAL_PHOTOS_FOLDER) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "for fname in tqdm(local_files, desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤\"):\n",
    "    file_path = os.path.join(LOCAL_PHOTOS_FOLDER, fname)\n",
    "    gfile = drive.CreateFile({\n",
    "        'title': fname,\n",
    "        'parents': [{'id': FOLDER_ID}]\n",
    "    })\n",
    "    gfile.SetContentFile(file_path)\n",
    "    gfile.Upload()\n",
    "\n",
    "print(\"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77416cd1-04d2-4e4b-ad4f-5b632f1ce0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6be3a3-4f77-4f97-b2b4-34b526fc6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 7. Connect to Google Drive and get new filenames with their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4888431e-dc79-4438-bc86-68a8811537b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=849244129200-hdfqohk1rs46hjekajgu7pa4jqrn9sqj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n",
      "üîç –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ Google Drive...\n",
      "üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: 2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [00:00<00:00, 545814.09—Ñ–∞–π–ª/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –°—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ csv/google_drive_links.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===\n",
    "FOLDER_ID = '1em81MElkxnaue5r92e9hPGezgOuw4nRL'\n",
    "OUTPUT_CSV = 'csv/google_drive_links.csv'\n",
    "\n",
    "# === –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Google Drive ===\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# === –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ===\n",
    "print(\"üîç –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ Google Drive...\")\n",
    "file_list = drive.ListFile({'q': f\"'{FOLDER_ID}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "# === –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º ===\n",
    "data = []\n",
    "print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_list)}\")\n",
    "\n",
    "for file in tqdm(file_list, desc=\"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤\", unit=\"—Ñ–∞–π–ª\"):\n",
    "    filename = file['title']\n",
    "    file_id = file['id']\n",
    "    view_url = f\"https://drive.google.com/uc?export=view&id={file_id}\"\n",
    "    data.append({\n",
    "        'filename': filename,\n",
    "        'file_id': file_id,\n",
    "        'link': view_url\n",
    "    })\n",
    "\n",
    "# === –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV ===\n",
    "os.makedirs('csv', exist_ok=True)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ –°—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fdaed-4496-473c-874a-d054d08d11ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ab8ba-8fd5-474c-80d0-8d14716005fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 8. Get all info from photos_combined.csv and google_drive_links.csv into photos_final.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7daffb8-1401-4699-af02-647e16255385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV...\n",
      "\n",
      "‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: csv/photos_final.csv\n",
      "üìÑ –°—Ç—Ä–æ–∫:  2431\n",
      "üß∑ –ö–æ–ª–æ–Ω–∫–∏:  ['filename', 'folder', 'latitude', 'longitude', 'year', 'month', 'day', 'album', 'source_filename', 'source_type', 'description', 'link']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === –ü—É—Ç–∏ ===\n",
    "CSV_FOLDER = \"csv\"\n",
    "COMBINED_CSV = os.path.join(CSV_FOLDER, \"photos_combined.csv\")\n",
    "LINKS_CSV = os.path.join(CSV_FOLDER, \"google_drive_links.csv\")\n",
    "FINAL_CSV = os.path.join(CSV_FOLDER, \"photos_final.csv\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===\n",
    "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV...\")\n",
    "df_combined = pd.read_csv(COMBINED_CSV)\n",
    "df_links = pd.read_csv(LINKS_CSV)\n",
    "\n",
    "# === –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ filename ===\n",
    "df_final = df_combined.merge(df_links[['filename', 'link']], on='filename', how='left')\n",
    "\n",
    "# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ ===\n",
    "df_final.to_csv(FINAL_CSV, index=False)\n",
    "\n",
    "# === –û—Ç—á—ë—Ç ===\n",
    "print(\"\\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω:\", FINAL_CSV)\n",
    "print(\"üìÑ –°—Ç—Ä–æ–∫: \", len(df_final))\n",
    "print(\"üß∑ –ö–æ–ª–æ–Ω–∫–∏: \", list(df_final.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574432e0-ce62-4b19-9ac1-9fb4b548bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9. Put all info from photos_final.csv into photos.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd8e9259-a69b-4e27-b700-37a230173f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç GeoJSON —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω: geojson/photos.geojson\n",
      "üìç –¢–æ—á–µ–∫:  2431\n",
      "üìÑ –ö–æ–ª–æ–Ω–∫–∏:  ['filename', 'folder', 'latitude', 'longitude', 'year', 'month', 'day', 'album', 'source_filename', 'source_type', 'description', 'geometry', 'image', 'fullname']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# === –ü—É—Ç–∏ ===\n",
    "CSV_FOLDER = \"csv\"\n",
    "GEOJSON_FOLDER = \"geojson\"\n",
    "FINAL_CSV = os.path.join(CSV_FOLDER, \"photos_final.csv\")\n",
    "GEOJSON_OUTPUT = os.path.join(GEOJSON_FOLDER, \"photos.geojson\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===\n",
    "df = pd.read_csv(FINAL_CSV)\n",
    "\n",
    "# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ ===\n",
    "df_geo = df[df['latitude'].notnull() & df['longitude'].notnull()].copy()\n",
    "\n",
    "# === –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ ===\n",
    "df_geo['geometry'] = df_geo.apply(lambda row: Point(float(row['longitude']), float(row['latitude'])), axis=1)\n",
    "\n",
    "# === –î–æ–±–∞–≤–ª–µ–Ω–∏–µ image –∏ fullname –≤–º–µ—Å—Ç–æ link ===\n",
    "def make_drive_urls(link):\n",
    "    if isinstance(link, str) and \"id=\" in link:\n",
    "        file_id = link.split(\"id=\")[-1]\n",
    "        return pd.Series({\n",
    "            \"image\": f\"https://drive.google.com/thumbnail?id={file_id}\",\n",
    "            \"fullname\": f\"https://drive.google.com/uc?export=view&id={file_id}\"\n",
    "        })\n",
    "    return pd.Series({\"image\": None, \"fullname\": None})\n",
    "\n",
    "url_cols = df_geo[\"link\"].apply(make_drive_urls)\n",
    "df_geo = pd.concat([df_geo.drop(columns=[\"link\"]), url_cols], axis=1)\n",
    "\n",
    "# === –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ GeoDataFrame –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===\n",
    "gdf = gpd.GeoDataFrame(df_geo, geometry='geometry', crs='EPSG:4326')\n",
    "os.makedirs(GEOJSON_FOLDER, exist_ok=True)\n",
    "gdf.to_file(GEOJSON_OUTPUT, driver='GeoJSON')\n",
    "\n",
    "# === –û—Ç—á—ë—Ç ===\n",
    "print(\"\\nüåç GeoJSON —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω:\", GEOJSON_OUTPUT)\n",
    "print(\"üìç –¢–æ—á–µ–∫: \", len(gdf))\n",
    "print(\"üìÑ –ö–æ–ª–æ–Ω–∫–∏: \", list(gdf.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a911c3-7940-44e1-8152-d8a75ed7d651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8ce6d-c43a-4eb0-bdb4-05796688ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 10. Get country code from nominate site into photos.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "999c4ffd-45e6-4dbc-8c24-6c24b0302266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåç –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–µ–∫: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [1:31:56<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –§–∞–π–ª geojson/photos.geojson –æ–±–Ω–æ–≤–ª—ë–Ω —Å country_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "GEOJSON_FILE = \"geojson/photos.geojson\"\n",
    "NOMINATIM_URL = \"https://nominatim.openstreetmap.org/reverse\"\n",
    "DELAY = 2  # —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'PhotoMapsApp (bizbur08@gmail.com)'  # üîß –£–∫–∞–∂–∏ —Å–≤–æ–π email!\n",
    "}\n",
    "\n",
    "# ‚Äî –ö—ç—à –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã\n",
    "country_cache = {}\n",
    "\n",
    "def get_country_code(lat, lon):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º —á–µ—Ä–µ–∑ Nominatim API\n",
    "    \"\"\"\n",
    "    key = f\"{lat:.4f},{lon:.4f}\"\n",
    "    if key in country_cache:\n",
    "        return country_cache[key]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(NOMINATIM_URL, params={\n",
    "            'format': 'json',\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }, headers=HEADERS)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            code = data.get('address', {}).get('country_code', '').upper()\n",
    "            country_cache[key] = code if code else '??'\n",
    "            return country_cache[key]\n",
    "        else:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –¥–ª—è {lat},{lon}\")\n",
    "            return '??'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è {lat},{lon}: {e}\")\n",
    "        return '??'\n",
    "\n",
    "def enrich_geojson():\n",
    "    \"\"\"\n",
    "    –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω—ã –≤ –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É GeoJSON\n",
    "    \"\"\"\n",
    "    with open(GEOJSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    for feature in tqdm(geojson_data[\"features\"], desc=\"üåç –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–µ–∫\"):\n",
    "        lat, lon = feature[\"geometry\"][\"coordinates\"][1], feature[\"geometry\"][\"coordinates\"][0]\n",
    "        country_code = get_country_code(lat, lon)\n",
    "        feature[\"properties\"][\"country_code\"] = country_code\n",
    "        time.sleep(DELAY)\n",
    "\n",
    "    with open(GEOJSON_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(geojson_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ –§–∞–π–ª {GEOJSON_FILE} –æ–±–Ω–æ–≤–ª—ë–Ω —Å country_code\")\n",
    "\n",
    "# === –ó–∞–ø—É—Å–∫ ===\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_geojson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe034ed9-2119-4be9-9fcc-8651b684feb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
