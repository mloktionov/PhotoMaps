{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4c3cd3-3dca-4cca-a622-3650e5fba878",
   "metadata": {},
   "source": [
    "–í–æ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –¥–≤—É—Ö —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ PhotoMaps:\n",
    "\n",
    "üõ†Ô∏è –°–∫—Ä–∏–ø—Ç 1: –°–∂–∞—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è CSV (compress_and_index.py)\n",
    "\n",
    "–ó–∞–¥–∞—á–∞:\n",
    "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ images/:\n",
    "\t‚Ä¢\t–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –¥–∞—Ç—É (EXIF + JSON)\n",
    "\t‚Ä¢\t–ï—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –µ—Å—Ç—å ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∂–∞—Ç—É—é –∫–æ–ø–∏—é –≤ photos/\n",
    "\t‚Ä¢\t–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ —à–∞–±–ª–æ–Ω—É IMG_YYYYMMDD_hash.jpg, –µ—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞ –≤ –∏–º–µ–Ω–∏\n",
    "\t‚Ä¢\t–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∏–º—è, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –¥–∞—Ç–∞) –≤ csv/photos.csv\n",
    "\t‚Ä¢\t–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç photos.csv –Ω–∞ —Ñ–∞–π–ª—ã –ø–æ –≥–æ–¥–∞–º: photos_2020.csv, photos_2021.csv –∏ —Ç.–¥.\n",
    "\t‚Ä¢\t–í—Å–µ –æ—Ç–±—Ä–∞–∫–æ–≤–∫–∏ (–±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–ª–∏ –æ—à–∏–±–æ–∫) ‚Äî –≤ csv/errors_combined.csv\n",
    "\n",
    "–í—Ö–æ–¥:\n",
    "\t‚Ä¢\timages/*.jpg|.jpeg\n",
    "\t‚Ä¢\t(–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) .json —Ä—è–¥–æ–º —Å —Ñ–æ—Ç–æ (.supplemental-metadata.json)\n",
    "\n",
    "–í—ã—Ö–æ–¥:\n",
    "\t‚Ä¢\tphotos/*.jpg ‚Äî —Å–∂–∞—Ç—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏\n",
    "\t‚Ä¢\tcsv/photos.csv ‚Äî –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "\t‚Ä¢\tcsv/photos_YYYY.csv ‚Äî —Ä–∞–∑–±–∏—Ç—ã–µ –ø–æ –≥–æ–¥–∞–º\n",
    "\t‚Ä¢\tcsv/errors_combined.csv ‚Äî –ª–æ–≥ –æ—à–∏–±–æ–∫\n",
    "\n",
    "üé® –°–∫—Ä–∏–ø—Ç 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏–Ω–∏–∞—Ç—é—Ä (generate_thumbnails.py)\n",
    "\n",
    "–ó–∞–¥–∞—á–∞:\n",
    "–°–æ–∑–¥–∞–µ—Ç –∫—Ä—É–≥–ª—ã–µ –º–∏–Ω–∏–∞—Ç—é—Ä—ã 64x64 PNG –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∂–∞—Ç–æ–≥–æ —Ñ–æ—Ç–æ –∏–∑ photos/, –æ–∫—Ä–∞—à–∏–≤–∞–µ—Ç —Ä–∞–º–∫—É –ø–æ –≥–æ–¥—É.\n",
    "\n",
    "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\n",
    "\t‚Ä¢\t–ü—Ä–µ–≤—å—é –æ—Ñ–æ—Ä–º–ª–µ–Ω–æ –∫–∞–∫ –∫—Ä—É–≥ —Å —Ü–≤–µ—Ç–Ω–æ–π —Ä–∞–º–∫–æ–π\n",
    "\t‚Ä¢\t–¶–≤–µ—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ–¥–∞ (–∏–∑ –ø–∞–ª–∏—Ç—Ä—ã)\n",
    "\t‚Ä¢\t–ù–∞–∑–≤–∞–Ω–∏—è .jpg/.jpeg ‚Üí .png\n",
    "\n",
    "–í—Ö–æ–¥:\n",
    "\t‚Ä¢\tphotos/*.jpg\n",
    "\t‚Ä¢\tcsv/photos.csv (–¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–æ–¥–∞)\n",
    "\n",
    "–í—ã—Ö–æ–¥:\n",
    "\t‚Ä¢\tthumbnails/*.png ‚Äî –∏–∫–æ–Ω–∫–∏, –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ –Ω–∞ –∫–∞—Ä—Ç–µ\n",
    "\n",
    "üîÑ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "\t1.\timages/ ‚Äî –∏—Å—Ö–æ–¥–Ω—ã–µ JPG-—Ñ–∞–π–ª—ã (–∏ .json –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏)\n",
    "\t2.\t‚ñ∂Ô∏è –°–∫—Ä–∏–ø—Ç 1: compress_and_index.py\n",
    "\t‚Ä¢\t‚û°Ô∏è photos/ (JPEG, –∫–æ–º–ø—Ä–µ—Å—Å–∏—è)\n",
    "\t‚Ä¢\t‚û°Ô∏è csv/ (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)\n",
    "\t3.\t‚ñ∂Ô∏è –°–∫—Ä–∏–ø—Ç 2: generate_thumbnails.py\n",
    "\t‚Ä¢\t‚û°Ô∏è thumbnails/ (–∏–∫–æ–Ω–∫–∏ PNG)\n",
    "\t4.\tüó∫Ô∏è –ö–∞—Ä—Ç–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–æ—á–∫–∏ —Å –ø—Ä–µ–≤—å—é –∏ –ø–æ–ø–∞–ø–∞–º–∏, –¥–∞–Ω–Ω—ã–µ –∏–∑ csv/photos_*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7142a54-d80e-48b5-b9e1-f06db72ba5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "–ü–∞–π–ø–ª–∞–π–Ω\n",
    "=======\n",
    "\n",
    "0. –í–°–ï —Ñ–æ—Ç–æ –ø–æ–ª–æ–∂–∏—Ç—å –≤ –ø–∞–ø–∫—É images/\n",
    "\n",
    "PhotoMaps.ipynb - \n",
    "1. script to study our images. Retrieves how many files of which types are there in source folder images/--\n",
    "\n",
    "OUTPUT\n",
    "üìÅ –ê–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏: images\n",
    "===================================\n",
    "  .JPG: 2185\n",
    " .JPEG: 20\n",
    " .HEIC: 6\n",
    " .JSON: 2068\n",
    "üåÄ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å (1), (2)...: 5\n",
    "üìã –î—É–±–ª–∏–∫–∞—Ç–æ–≤ copy         : 4\n",
    "‚úÇÔ∏è  -edited —Ñ–∞–π–ª–æ–≤         : 146\n",
    "===================================\n",
    "üì¶ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: 4282\n",
    "\n",
    "2. # -- duplicate images eliminator -- run every time you download the new copy of google photos\n",
    "# -- file type: IMG_1234(1).JPG, IMG_1234(2).JPG and their json (if any)\n",
    "\n",
    "OUTPUT\n",
    "üîç –ù–∞–π–¥–µ–Ω—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ JSON —Ñ–∞–π–ª—ã:\n",
    " - EFFECTS.jpg.supplemental-metadata(1).json\n",
    " - IMG_2822.JPG.supplemental-metadata(1).json\n",
    " - IMG_0128.JPG.supplemental-metadata(1).json\n",
    "\n",
    "3. # -- duplicate images eliminator -- run every time you download the new copy of google photos\n",
    "# -- file type: IMG_1234 copy.JPG, IMG_1234 copy copy.JPG and their json (if any)\n",
    "\n",
    "=== üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò ===\n",
    "üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤-–∫–æ–ø–∏–π: 4\n",
    "üìå –§–∞–π–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö EXIF: 0\n",
    "‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ JSON —Ñ–∞–π–ª–æ–≤: 4\n",
    "‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –±–∏—Ç–º–∞–ø–æ–≤: 0\n",
    "\n",
    "4. # -- scripts to check HEIC files  -- \n",
    "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ .HEIC –≤ –ø–∞–ø–∫–µ images\n",
    "–ù–∞–π–¥–µ–Ω–æ: 6 HEIC —Ñ–∞–π–ª–æ–≤\n",
    "‚ùå –ë–µ–∑ 'ftyp': 6\n",
    " - IMG_1894.HEIC\n",
    " - IMG_1899.HEIC\n",
    "\n",
    "5. # -- HEIC > JPG converter\n",
    "‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏ —É–¥–∞–ª–µ–Ω–æ: 6 HEIC ‚Üí JPG\n",
    "\n",
    "6. # --- images>photos converter  - MAIN script --\n",
    "‚úÖ –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: 2065 ‚Üí /Users/mloktionov/PycharmProjects/PhotoMaps/csv/photos.csv\n",
    "‚ö†Ô∏è –ë–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç : 0 ‚Üí /Users/mloktionov/PycharmProjects/PhotoMaps/csv/missing_coords.csv\n",
    "üìÅ –§–æ—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: /Users/mloktionov/PycharmProjects/PhotoMaps/photos\n",
    "üìù –õ–æ–≥ –∑–∞–ø–∏—Å–∞–Ω –≤: /Users/mloktionov/PycharmProjects/PhotoMaps/csv/import_log.txt\n",
    "üåç –î–∂–∏—Ç—Ç–µ—Ä –ø—Ä–∏–º–µ–Ω—ë–Ω –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç\n",
    "\n",
    "‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ photos.csv –∏ missing_coords.csv\n",
    "\n",
    "7. # ------ Generating preview thumbnails in circles of a certain color (now OBSOLETE)\n",
    "\n",
    "(—É–∂–µ –Ω–µ –Ω—É–∂–µ–Ω)\n",
    "\n",
    "‚úÖ –ü—Ä–µ–≤—å—é —Å–æ–∑–¥–∞–Ω–æ: 2065\n",
    "‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: 0\n",
    "üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: /Users/mloktionov/PycharmProjects/PhotoMaps/thumbnails\n",
    "\n",
    "8. # ---- Getting descriptions from Google photos via API\n",
    "\n",
    "üîç –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ –≤ Google Photos...\n",
    "‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–ª—å–±–æ–º—ã: ['PhotoMap 2022-2025', 'PhotoMap 2019-2021', 'PhotoMap 2017-2019', 'PhotoMap 2014-2016']\n",
    "\n",
    "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2022-2025\n",
    "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2022-2025: 653—Ñ–æ—Ç–æ [00:07, 89.61—Ñ–æ—Ç–æ/s] \n",
    "\n",
    "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2019-2021\n",
    "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2019-2021: 784—Ñ–æ—Ç–æ [00:07, 100.08—Ñ–æ—Ç–æ/s]\n",
    "\n",
    "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2017-2019\n",
    "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2017-2019: 249—Ñ–æ—Ç–æ [00:02, 95.38—Ñ–æ—Ç–æ/s] \n",
    "\n",
    "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2014-2016\n",
    "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2014-2016: 381—Ñ–æ—Ç–æ [00:05, 75.50—Ñ–æ—Ç–æ/s]\n",
    "\n",
    "‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ csv/photos_descriptions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566076a1-38ca-4b47-ab05-80e67e82d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- script to study our images. Retrieves how many files of which types are there in source folder images/--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60947472-f465-451f-ab86-2c9f83e42385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "FOLDER = \"images\"  # –∑–∞–º–µ–Ω–∏ –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "files = os.listdir(FOLDER)\n",
    "\n",
    "# === –†–∞—Å—à–∏—Ä–µ–Ω–∏—è\n",
    "ext_counts = Counter()\n",
    "suffix_counts = Counter()\n",
    "edited_count = 0\n",
    "copy_count = 0\n",
    "\n",
    "for f in files:\n",
    "    lower = f.lower()\n",
    "    if lower.endswith(('.jpg', '.jpeg', '.heic')):\n",
    "        ext = os.path.splitext(f)[1].lower()\n",
    "        ext_counts[ext] += 1\n",
    "\n",
    "        # –¥—É–±–ª–∏–∫–∞—Ç—ã: IMG_1234(1).JPG\n",
    "        if re.search(r'\\(\\d+\\)\\.(jpe?g|heic)$', lower):\n",
    "            suffix_counts['duplicate'] += 1\n",
    "\n",
    "        # -edited.JPG\n",
    "        if '-edited' in lower:\n",
    "            edited_count += 1\n",
    "\n",
    "        # \" copy\" –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "        if re.search(r' copy(\\s\\d+)?\\.(jpe?g|heic)$', lower):\n",
    "            copy_count += 1\n",
    "\n",
    "    elif lower.endswith('.json'):\n",
    "        ext_counts['.json'] += 1\n",
    "\n",
    "# === –í—ã–≤–æ–¥\n",
    "print(f\"üìÅ –ê–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏: {FOLDER}\")\n",
    "print(\"===================================\")\n",
    "for ext in ['.jpg', '.jpeg', '.heic', '.json']:\n",
    "    print(f\"{ext.upper():>6}: {ext_counts[ext]}\")\n",
    "\n",
    "print(f\"üåÄ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å (1), (2)...: {suffix_counts['duplicate']}\")\n",
    "print(f\"üìã –î—É–±–ª–∏–∫–∞—Ç–æ–≤ copy         : {copy_count}\")\n",
    "print(f\"‚úÇÔ∏è  -edited —Ñ–∞–π–ª–æ–≤         : {edited_count}\")\n",
    "print(\"===================================\")\n",
    "print(f\"üì¶ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4f84a-e0e6-4cf6-86c8-de983df9aea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- duplicate images eliminator -- run every time you download the new copy of google photos\n",
    "# -- file type: IMG_1234(1).JPG, IMG_1234(2).JPG and their json (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee59d9-8744-44b4-86b1-05e00d2ebbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# === üìÅ –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "folder = \"images\"\n",
    "\n",
    "# === üéØ –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö JSON\n",
    "pattern = re.compile(r\"(.*)\\.supplemental-metadata\\((\\d+)\\)\\.json\", re.IGNORECASE)\n",
    "\n",
    "# === üîÑ –ü–æ–∏—Å–∫ –≤—Å–µ—Ö JSON —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º\n",
    "json_files = [f for f in os.listdir(folder) if pattern.match(f)]\n",
    "\n",
    "# === üîé –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "if json_files:\n",
    "    print(f\"üîç –ù–∞–π–¥–µ–Ω—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ JSON —Ñ–∞–π–ª—ã:\")\n",
    "    for f in json_files:\n",
    "        print(f\" - {f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è!\")\n",
    "\n",
    "for fname in json_files:\n",
    "    json_path = os.path.join(folder, fname)\n",
    "    \n",
    "    # === ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {json_path} –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º!\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_path}\")\n",
    "    \n",
    "    # === üîç –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n",
    "    match = pattern.match(fname)\n",
    "    base_name, index = match.groups()\n",
    "    print(f\"   ‚Üí –ë–∞–∑–æ–≤–æ–µ –∏–º—è: {base_name}, –ò–Ω–¥–µ–∫—Å: {index}\")\n",
    "    \n",
    "    # === üìù –ß–∏—Å—Ç–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–ª–∏—Å—å\n",
    "    if base_name.lower().endswith('.jpg'):\n",
    "        base_name = base_name[:-4]\n",
    "    elif base_name.lower().endswith('.jpeg'):\n",
    "        base_name = base_name[:-5]\n",
    "\n",
    "    # === üìù –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è\n",
    "    correct_name = f\"{base_name}({index}).JPG.supplemental-metadata.json\"\n",
    "    correct_path = os.path.join(folder, correct_name)\n",
    "    \n",
    "    print(f\"üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {json_path} ‚Üí {correct_path}\")\n",
    "    \n",
    "    # === üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ\n",
    "    try:\n",
    "        os.rename(json_path, correct_path)\n",
    "        \n",
    "        # === ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –ø–æ—è–≤–∏–ª—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è\n",
    "        if not os.path.exists(correct_path):\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {correct_path} –Ω–µ –ø–æ—è–≤–∏–ª—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è!\")\n",
    "            continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # === üîÑ –ß–∏—Ç–∞–µ–º JSON –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º title\n",
    "    try:\n",
    "        with open(correct_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            metadata = json.load(file)\n",
    "        \n",
    "        # === –ù–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è title\n",
    "        old_title = metadata.get(\"title\", \"\")\n",
    "        new_title = f\"{base_name}({index}).JPG\"\n",
    "        \n",
    "        if old_title != new_title:\n",
    "            print(f\"üîÑ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ title: {old_title} ‚Üí {new_title}\")\n",
    "            metadata[\"title\"] = new_title\n",
    "            \n",
    "            # === üìù –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ\n",
    "            with open(correct_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(metadata, file, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(f\"‚úÖ Title —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π: {old_title}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON: {correct_path} ‚Üí {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d88ca-b061-40d8-b452-f2da4ac44dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- duplicate images eliminator -- run every time you download the new copy of google photos\n",
    "# -- file type: IMG_1234 copy.JPG, IMG_1234 copy copy.JPG and their json (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba02a70-8a09-4aee-9dec-22cc483725b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# === üìÅ –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "folder = \"images\"\n",
    "\n",
    "# === üóÇÔ∏è –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞\n",
    "if not os.path.exists(folder):\n",
    "    print(f\"‚ùå –ü–∞–ø–∫–∞ '{folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞—é –µ—ë...\")\n",
    "    os.makedirs(folder)\n",
    "    print(f\"‚úÖ –ü–∞–ø–∫–∞ '{folder}' —Å–æ–∑–¥–∞–Ω–∞. –ü–æ–º–µ—Å—Ç–∏ –≤ –Ω–µ—ë —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\")\n",
    "    exit()\n",
    "\n",
    "# === üëÅÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ GPS –≤ EXIF\n",
    "def has_gps(exif):\n",
    "    gps_tag_id = [k for k, v in ExifTags.TAGS.items() if v == \"GPSInfo\"]\n",
    "    if not gps_tag_id:\n",
    "        return False\n",
    "    gps_data = exif.get(gps_tag_id[0])\n",
    "    return bool(gps_data)\n",
    "\n",
    "# === üéØ –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–ø–æ—á–µ–∫ \" copy\"\n",
    "pattern = re.compile(r\"^(.+?)( copy)+(\\s\\d+)?(\\..+)$\", re.IGNORECASE)\n",
    "candidates = [f for f in os.listdir(folder) if pattern.search(f) and f.lower().endswith(('.jpg', '.jpeg', '.png', '.heic'))]\n",
    "\n",
    "# === üóÇÔ∏è –°–ø–∏—Å–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "not_found_json = []\n",
    "updated_json_files = []\n",
    "missing_gps_and_json = []\n",
    "files_with_gps = []\n",
    "\n",
    "print(f\"üîé –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å –º–µ—Ç–∫–æ–π ' copy': {len(candidates)}\")\n",
    "\n",
    "for fname in candidates:\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    original_base = base.replace(' copy', '')\n",
    "    \n",
    "    # === üîÑ –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ JSON –¥–ª—è –≤—Å–µ—Ö —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤\n",
    "    json_candidates = [\n",
    "        f\"{original_base}.jpg.supplemental-metadata copy.json\",\n",
    "        f\"{original_base}.jpeg.supplemental-metadata copy.json\",\n",
    "        f\"{original_base}.JPG.supplemental-metadata copy.json\",\n",
    "        f\"{original_base}.JPEG.supplemental-metadata copy.json\",\n",
    "    ]\n",
    "    \n",
    "    # === üîÑ –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ JSON\n",
    "    existing_jsons = [j for j in json_candidates if os.path.exists(os.path.join(folder, j))]\n",
    "    \n",
    "    if existing_jsons:\n",
    "        json_path = os.path.join(folder, existing_jsons[0])\n",
    "        print(f\"\\n‚úÖ JSON –Ω–∞–π–¥–µ–Ω –¥–ª—è: {fname} ‚Üí {existing_jsons[0]}\")\n",
    "\n",
    "        # === üìù –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "        new_json_name = f\"{fname}.supplemental-metadata.json\"\n",
    "        new_json_path = os.path.join(folder, new_json_name)\n",
    "        os.rename(json_path, new_json_path)\n",
    "\n",
    "        # === üîÑ –û–±–Ω–æ–≤–ª—è–µ–º \"title\"\n",
    "        with open(new_json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            metadata = json.load(file)\n",
    "\n",
    "        original_title = metadata.get(\"title\", \"\")\n",
    "        if original_title != fname:\n",
    "            print(f\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ 'title' –≤ JSON: {original_title} ‚Üí {fname}\")\n",
    "            metadata[\"title\"] = fname\n",
    "        \n",
    "            # === üìù –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ\n",
    "            with open(new_json_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(metadata, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        updated_json_files.append(fname)\n",
    "        print(f\"‚úÖ JSON –¥–ª—è {fname} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω ‚Üí {new_json_path}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è: {fname}\")\n",
    "        not_found_json.append(fname)\n",
    "        continue\n",
    "\n",
    "    # === ‚úÖ 1. –ï—Å–ª–∏ –µ—Å—Ç—å EXIF ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n",
    "    img_path = os.path.join(folder, fname)\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        exif = img._getexif() or {}\n",
    "        if has_gps(exif):\n",
    "            files_with_gps.append(fname)\n",
    "            print(f\"üåç EXIF-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–π–¥–µ–Ω—ã –≤ {fname}, JSON –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        missing_gps_and_json.append(f\"{fname} (–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è EXIF: {e})\")\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è EXIF: {fname} ‚Üí {e}\")\n",
    "        continue\n",
    "\n",
    "# === üìù –ò—Ç–æ–≥–∏\n",
    "print(\"\\n=== üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò ===\")\n",
    "print(f\"üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤-–∫–æ–ø–∏–π: {len(candidates)}\")\n",
    "print(f\"üìå –§–∞–π–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö EXIF: {len(files_with_gps)}\")\n",
    "print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ JSON —Ñ–∞–π–ª–æ–≤: {len(updated_json_files)}\")\n",
    "print(f\"‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –±–∏—Ç–º–∞–ø–æ–≤: {len(not_found_json)}\")\n",
    "\n",
    "# === üìÇ –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤-–∫–æ–ø–∏–π\n",
    "print(\"\\nüìÇ –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤-–∫–æ–ø–∏–π:\")\n",
    "for f in candidates:\n",
    "    print(f\" - {f}\")\n",
    "\n",
    "if not_found_json:\n",
    "    print(\"\\n‚õî –°–ø–∏—Å–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö JSON –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:\")\n",
    "    for f in not_found_json:\n",
    "        print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9e870-10cf-4b31-b1b8-6f371e9e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scripts to convert HEIC files into jpgs -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd244686-9945-4f67-8330-e39a304ddf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# üìÅ –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "folder = \"images\"\n",
    "\n",
    "# üîç –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ .HEIC —Ñ–∞–π–ª—ã\n",
    "heic_files = [f for f in os.listdir(folder) if f.lower().endswith(\".heic\")]\n",
    "\n",
    "invalid_files = []\n",
    "\n",
    "for fname in heic_files:\n",
    "    path = os.path.join(folder, fname)\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            header = f.read(512)\n",
    "            if b\"ftyp\" not in header:\n",
    "                invalid_files.append(fname)\n",
    "    except Exception as e:\n",
    "        invalid_files.append(f\"{fname} (–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e})\")\n",
    "\n",
    "# üìã –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "print(f\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ .HEIC –≤ –ø–∞–ø–∫–µ {folder}\")\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ: {len(heic_files)} HEIC —Ñ–∞–π–ª–æ–≤\")\n",
    "print(f\"‚ùå –ë–µ–∑ 'ftyp': {len(invalid_files)}\")\n",
    "for f in invalid_files:\n",
    "    print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83b9f5-9ce2-4bc3-854e-f886b6d4f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HEIC > JPG converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537533b6-bfc4-460f-993f-483d650b920e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# === üìÅ –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "folder = \"images\"\n",
    "\n",
    "# === üîç –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ HEIC —Ñ–∞–π–ª—ã\n",
    "heic_files = [f for f in os.listdir(folder) if f.lower().endswith(\".heic\")]\n",
    "converted = []\n",
    "failed = []\n",
    "\n",
    "# === üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HEIC ‚Üí JPG –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ JSON\n",
    "for fname in heic_files:\n",
    "    in_path = os.path.join(folder, fname)\n",
    "    out_name = os.path.splitext(fname)[0] + \".jpg\"\n",
    "    out_path = os.path.join(folder, out_name)\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"sips\", \"-s\", \"format\", \"jpeg\", in_path, \"--out\", out_path],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            # üóëÔ∏è –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π HEIC\n",
    "            os.remove(in_path)\n",
    "            converted.append(out_name)\n",
    "\n",
    "            # === üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ JSON\n",
    "            json_name = f\"{os.path.splitext(fname)[0]}.HEIC.supplemental-metadata.json\"\n",
    "            new_json_name = f\"{os.path.splitext(fname)[0]}.JPG.supplemental-metadata.json\"\n",
    "\n",
    "            json_path = os.path.join(folder, json_name)\n",
    "            new_json_path = os.path.join(folder, new_json_name)\n",
    "\n",
    "            if os.path.exists(json_path):\n",
    "                # === üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ JSON\n",
    "                os.rename(json_path, new_json_path)\n",
    "                print(f\"üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω JSON: {json_name} ‚Üí {new_json_name}\")\n",
    "\n",
    "                # === üìù –ò—Å–ø—Ä–∞–≤–ª—è–µ–º \"title\" –≤–Ω—É—Ç—Ä–∏ JSON\n",
    "                try:\n",
    "                    with open(new_json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        metadata = json.load(file)\n",
    "\n",
    "                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–µ \"title\"\n",
    "                    metadata[\"title\"] = out_name\n",
    "\n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ\n",
    "                    with open(new_json_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                        json.dump(metadata, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "                    print(f\"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω 'title' –≤ JSON: {out_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ JSON {new_json_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_name}\")\n",
    "        else:\n",
    "            failed.append((fname, result.stderr.strip()))\n",
    "    except Exception as e:\n",
    "        failed.append((fname, str(e)))\n",
    "\n",
    "# === üìù –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(f\"\\n‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏ —É–¥–∞–ª–µ–Ω–æ: {len(converted)} HEIC ‚Üí JPG\")\n",
    "if failed:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∏: {len(failed)}\")\n",
    "    for fname, msg in failed:\n",
    "        print(f\" - {fname}: {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b9358-c6ad-47d0-b94b-6742515604e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- images>photos converter  - MAIN script --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d48c0-6e05-44b1-9537-681024cc4289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ExifTags\n",
    "import pillow_heif\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import math\n",
    "\n",
    "# === üìÅ –ü–∞–ø–∫–∏\n",
    "ROOT = os.path.abspath(\".\")\n",
    "SRC_FOLDER = os.path.join(ROOT, \"images\")\n",
    "OUT_FOLDER = os.path.join(ROOT, \"photos\")\n",
    "MISSING_FOLDER = os.path.join(ROOT, \"missing_coords\")\n",
    "CSV_FOLDER = os.path.join(ROOT, \"csv\")\n",
    "LOG_FILE = os.path.join(CSV_FOLDER, \"import_log.txt\")\n",
    "\n",
    "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(MISSING_FOLDER, exist_ok=True)\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "\n",
    "CSV_MAIN = os.path.join(CSV_FOLDER, \"photos.csv\")\n",
    "CSV_MISSING = os.path.join(CSV_FOLDER, \"missing_coords.csv\")\n",
    "\n",
    "# === EXIF utils\n",
    "def convert_to_degrees(v):\n",
    "    d, m, s = v\n",
    "    return float(d) + float(m) / 60 + float(s) / 3600\n",
    "\n",
    "def extract_gps(exif):\n",
    "    gps_raw = {}\n",
    "    for tag_id, val in exif.items():\n",
    "        tag = ExifTags.TAGS.get(tag_id)\n",
    "        if tag == \"GPSInfo\":\n",
    "            for key in val:\n",
    "                gps_tag = ExifTags.GPSTAGS.get(key)\n",
    "                gps_raw[gps_tag] = val[key]\n",
    "    if 'GPSLatitude' in gps_raw and 'GPSLongitude' in gps_raw:\n",
    "        try:\n",
    "            lat = convert_to_degrees(gps_raw['GPSLatitude'])\n",
    "            if gps_raw.get('GPSLatitudeRef', 'N') != 'N':\n",
    "                lat = -lat\n",
    "            lon = convert_to_degrees(gps_raw['GPSLongitude'])\n",
    "            if gps_raw.get('GPSLongitudeRef', 'E') != 'E':\n",
    "                lon = -lon\n",
    "            return lat, lon\n",
    "        except:\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "def extract_date(exif):\n",
    "    date_str = exif.get(36867) or exif.get(306)\n",
    "    if date_str:\n",
    "        match = re.match(r\"(\\d{4}):(\\d{2}):(\\d{2})\", date_str)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2), match.group(3)\n",
    "    return None, None, None\n",
    "\n",
    "Image.init()\n",
    "\n",
    "def spiral_coords(lat, lon, index, step=0.0002):\n",
    "    angle = index * (math.pi / 3)\n",
    "    radius = step * (1 + index // 6)\n",
    "    return lat + radius * math.cos(angle), lon + radius * math.sin(angle)\n",
    "\n",
    "image_files = [f for f in os.listdir(SRC_FOLDER)\n",
    "               if f.lower().endswith(('.jpg', '.jpeg', '.heic')) and \"-edited\" not in f.lower()]\n",
    "\n",
    "main_records = []\n",
    "missing_records = []\n",
    "log_entries = []\n",
    "\n",
    "for fname in tqdm(image_files, desc=\"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\", ncols=80):\n",
    "    in_path = os.path.join(SRC_FOLDER, fname)\n",
    "    ext = os.path.splitext(fname)[1].lower()\n",
    "\n",
    "    try:\n",
    "        if ext == \".heic\":\n",
    "            heif_file = pillow_heif.read_heif(in_path)\n",
    "            image = Image.frombytes(heif_file.mode, heif_file.size, heif_file.data, \"raw\")\n",
    "            exif = image.getexif()\n",
    "            log_entries.append(f\"[HEIC] Converted and loaded: {fname}\")\n",
    "        else:\n",
    "            image = Image.open(in_path)\n",
    "            exif = image._getexif() or {}\n",
    "            log_entries.append(f\"[IMG] Loaded: {fname}\")\n",
    "\n",
    "        lat, lon = extract_gps(exif)\n",
    "        year, month, day = extract_date(exif)\n",
    "        source_type = \"EXIF\"\n",
    "\n",
    "        json_candidates = glob.glob(in_path + \".supplemental*.json\")\n",
    "        if not json_candidates:\n",
    "            alt_path = in_path.replace(\".jpg\", \".JPG\")\n",
    "            json_candidates = glob.glob(alt_path + \".supplemental*.json\")\n",
    "        if not json_candidates:\n",
    "            alt_path = in_path.replace(\".jpeg\", \".JPEG\")\n",
    "            json_candidates = glob.glob(alt_path + \".supplemental*.json\")\n",
    "\n",
    "        if json_candidates:\n",
    "            try:\n",
    "                with open(json_candidates[0], \"r\", encoding=\"utf-8\") as jf:\n",
    "                    meta = json.load(jf)\n",
    "                if (lat is None or lon is None) and \"geoData\" in meta:\n",
    "                    lat = meta[\"geoData\"].get(\"latitude\")\n",
    "                    lon = meta[\"geoData\"].get(\"longitude\")\n",
    "                    if lat and lon:\n",
    "                        source_type = \"JSON\"\n",
    "                        log_entries.append(f\"[JSON] Used metadata for: {fname}\")\n",
    "            except Exception as e:\n",
    "                log_entries.append(f\"[ERROR] JSON read failed for {fname}: {e}\")\n",
    "                pass\n",
    "\n",
    "        if not (year and month and day):\n",
    "            year, month, day = \"2099\", \"01\", \"01\"\n",
    "\n",
    "        if not re.search(r'IMG_\\d{8}_', fname):\n",
    "            base_name = f\"IMG_{year}{month}{day}_{abs(hash(fname)) % 10**6}.jpg\"\n",
    "        else:\n",
    "            base_name = fname.replace(\".heic\", \".jpg\").replace(\".HEIC\", \".jpg\")\n",
    "\n",
    "        if lat and lon:\n",
    "            out_path = os.path.join(OUT_FOLDER, base_name)\n",
    "        else:\n",
    "            out_path = os.path.join(MISSING_FOLDER, base_name)\n",
    "\n",
    "        image.convert(\"RGB\").save(out_path, \"JPEG\", quality=85)\n",
    "        log_entries.append(f\"[SAVE] {fname} ‚Üí {out_path}\")\n",
    "\n",
    "        row = {\n",
    "            'filename': base_name,\n",
    "            'folder': os.path.basename(OUT_FOLDER if lat and lon else MISSING_FOLDER),\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'source_path': fname,\n",
    "            'source_type': source_type\n",
    "        }\n",
    "\n",
    "        if lat and lon:\n",
    "            main_records.append(row)\n",
    "        else:\n",
    "            missing_records.append(row)\n",
    "            log_entries.append(f\"[MISSING] {fname} ‚Üí No coordinates\")\n",
    "\n",
    "    except Exception as e:\n",
    "        missing_records.append({\n",
    "            'filename': fname,\n",
    "            'folder': os.path.basename(MISSING_FOLDER),\n",
    "            'latitude': None,\n",
    "            'longitude': None,\n",
    "            'year': None,\n",
    "            'month': None,\n",
    "            'day': None,\n",
    "            'error': str(e),\n",
    "            'source_path': fname,\n",
    "            'source_type': None\n",
    "        })\n",
    "        log_entries.append(f\"[ERROR] {fname}: {e}\")\n",
    "\n",
    "pd.DataFrame(main_records).to_csv(CSV_MAIN, index=False)\n",
    "pd.DataFrame(missing_records).to_csv(CSV_MISSING, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {len(main_records)} ‚Üí {CSV_MAIN}\")\n",
    "print(f\"‚ö†Ô∏è –ë–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç : {len(missing_records)} ‚Üí {CSV_MISSING}\")\n",
    "print(f\"üìÅ –§–æ—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_FOLDER} –∏ {MISSING_FOLDER}\")\n",
    "print(f\"üìù –õ–æ–≥ –∑–∞–ø–∏—Å–∞–Ω –≤: {LOG_FILE}\")\n",
    "print(\"üåç –î–∂–∏—Ç—Ç–µ—Ä –ø—Ä–∏–º–µ–Ω—ë–Ω –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç\")\n",
    "print(\"\\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ photos.csv –∏ missing_coords.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d657666-116e-471e-af10-017003a45925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67ebbf-b7c6-4998-9b56-f4ca452ce247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS\n",
    "\n",
    "# -- Add coordinates to missing_coords.csv and rename it into manual_coords.csv to include\n",
    "# remaining points\n",
    "# 1. Rename missing_coords into manual_coords\n",
    "# 2. Add coordinates (from manual_coords-Copy1.csv) or manually\n",
    "# 3. Check folder (should be photos)\n",
    "# 4. Run the script.\n",
    "# 5. Check the last lines of photos.csv to see if file names correspond to the ones in missing_coords/\n",
    "# 6. Move all files from missing_coords/ into photos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14a313-569a-4035-a3d8-ac6d48841336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CSV_MAIN = \"csv/photos.csv\"\n",
    "CSV_EXTRA = \"csv/manual_coords.csv\"\n",
    "CSV_INVALID = \"csv/manual_invalid.csv\"\n",
    "LOG_FILE = \"csv/merge_log.txt\"\n",
    "\n",
    "def log(message):\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤\n",
    "if not os.path.exists(CSV_MAIN):\n",
    "    raise FileNotFoundError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª: {CSV_MAIN}\")\n",
    "if not os.path.exists(CSV_EXTRA):\n",
    "    raise FileNotFoundError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª: {CSV_EXTRA}\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ CSV\n",
    "main_df = pd.read_csv(CSV_MAIN)\n",
    "extra_df = pd.read_csv(CSV_EXTRA)\n",
    "\n",
    "# === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "if set(main_df.columns) != set(extra_df.columns):\n",
    "    raise ValueError(\"‚ùå –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –º–µ–∂–¥—É CSV-—Ñ–∞–π–ª–∞–º–∏!\")\n",
    "\n",
    "# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç\n",
    "initial_len = len(extra_df)\n",
    "invalid_rows = extra_df[\n",
    "    extra_df[\"latitude\"].isna() |\n",
    "    extra_df[\"longitude\"].isna() |\n",
    "    (extra_df[\"latitude\"] == 0.0) |\n",
    "    (extra_df[\"longitude\"] == 0.0) |\n",
    "    (extra_df[\"latitude\"] < -90) | (extra_df[\"latitude\"] > 90) |\n",
    "    (extra_df[\"longitude\"] < -180) | (extra_df[\"longitude\"] > 180)\n",
    "]\n",
    "valid_extra_df = extra_df.drop(invalid_rows.index)\n",
    "\n",
    "# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã—Ö\n",
    "if not invalid_rows.empty:\n",
    "    invalid_rows.to_csv(CSV_INVALID, index=False)\n",
    "\n",
    "# === –õ–æ–≥\n",
    "log(\"üìã –õ–û–ì –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø\")\n",
    "log(f\"–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –≤ manual_coords.csv: {initial_len}\")\n",
    "log(f\"–î–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å—Ç—Ä–æ–∫: {len(valid_extra_df)}\")\n",
    "log(f\"–û—Ç–∫–ª–æ–Ω–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(invalid_rows)}\")\n",
    "\n",
    "# === –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "combined_df = pd.concat([main_df, valid_extra_df])\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"filename\"])\n",
    "\n",
    "# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "combined_df.to_csv(CSV_MAIN, index=False)\n",
    "log(f\"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf7353-3b67-4d0a-a1bc-a8a2b9d37e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Script deletes all files in Google Drive/Photos\n",
    "# --- Script uploads all files from photos/ to Google/Photos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f91472c-e053-47bb-9872-ff7de0262911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=849244129200-hdfqohk1rs46hjekajgu7pa4jqrn9sqj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n",
      "üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1460/1460 [13:38<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–∞–ø–∫–∞ –æ—á–∏—â–µ–Ω–∞.\n",
      "‚¨ÜÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ Google Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2431/2431 [3:10:57<00:00,  4.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# === –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# === ID —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–∏ –Ω–∞ Google Drive\n",
    "FOLDER_ID = '1em81MElkxnaue5r92e9hPGezgOuw4nRL'\n",
    "LOCAL_PHOTOS_FOLDER = 'photos/'\n",
    "\n",
    "# === –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive\n",
    "print(\"üßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ –Ω–∞ Google Drive...\")\n",
    "file_list = drive.ListFile({\n",
    "    'q': f\"'{FOLDER_ID}' in parents and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for f in tqdm(file_list, desc=\"–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\"):\n",
    "    f.Delete()\n",
    "\n",
    "print(\"‚úÖ –ü–∞–ø–∫–∞ –æ—á–∏—â–µ–Ω–∞.\")\n",
    "\n",
    "# === –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏\n",
    "print(\"‚¨ÜÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ Google Drive...\")\n",
    "local_files = [f for f in os.listdir(LOCAL_PHOTOS_FOLDER) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "for fname in tqdm(local_files, desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤\"):\n",
    "    file_path = os.path.join(LOCAL_PHOTOS_FOLDER, fname)\n",
    "    gfile = drive.CreateFile({\n",
    "        'title': fname,\n",
    "        'parents': [{'id': FOLDER_ID}]\n",
    "    })\n",
    "    gfile.SetContentFile(file_path)\n",
    "    gfile.Upload()\n",
    "\n",
    "print(\"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcabf53-b8cd-4e8c-a399-7049600d3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Getting descriptions from Google photos via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6e346e-9f1e-4b1c-80cb-16f542baa755",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=849244129200-hdfqohk1rs46hjekajgu7pa4jqrn9sqj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A60354%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fphotoslibrary.readonly&state=o3otxIUEcijTRIyPvoxZqfeQARifsC&access_type=offline\n",
      "üîç –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ –≤ Google Photos...\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–ª—å–±–æ–º—ã: ['PhotoMap 2022-2025', 'PhotoMap 2019-2021', 'PhotoMap 2017-2019', 'PhotoMap 2014-2016', 'PhotoMap 2010-2013', 'PhotoMap 08-09']\n",
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2022-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2022-2025: 653—Ñ–æ—Ç–æ [00:07, 87.67—Ñ–æ—Ç–æ/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2019-2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2019-2021: 784—Ñ–æ—Ç–æ [00:07, 101.69—Ñ–æ—Ç–æ/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2017-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2017-2019: 249—Ñ–æ—Ç–æ [00:02, 96.07—Ñ–æ—Ç–æ/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2014-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2014-2016: 381—Ñ–æ—Ç–æ [00:03, 98.38—Ñ–æ—Ç–æ/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 2010-2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 2010-2013: 257—Ñ–æ—Ç–æ [00:03, 83.82—Ñ–æ—Ç–æ/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ –ê–ª—å–±–æ–º: PhotoMap 08-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ PhotoMap 08-09: 109—Ñ–æ—Ç–æ [00:01, 72.67—Ñ–æ—Ç–æ/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ csv/photos_descriptions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build_from_document\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===\n",
    "SCOPES = ['https://www.googleapis.com/auth/photoslibrary.readonly']\n",
    "OUTPUT_CSV = 'csv/photos_descriptions.csv'\n",
    "ALBUM_NAMES = [\"PhotoMap 2022-2025\", \"PhotoMap 2019-2021\", \"PhotoMap 2017-2019\", \"PhotoMap 2014-2016\", \"PhotoMap 2010-2013\", \"PhotoMap 08-09\"]\n",
    "\n",
    "# === –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è ===\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    'client_secrets.json', SCOPES\n",
    ")\n",
    "credentials = flow.run_local_server(port=0)\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ discovery —Ñ–∞–π–ª–∞\n",
    "with open(\"photoslibrary_v1_discovery.json\", \"r\") as f:\n",
    "    discovery_doc = f.read()\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞\n",
    "service = build_from_document(discovery_doc, credentials=credentials)\n",
    "\n",
    "# === –ü–æ–∏—Å–∫ –Ω—É–∂–Ω—ã—Ö –∞–ª—å–±–æ–º–æ–≤ ===\n",
    "print(\"üîç –ü–æ–∏—Å–∫ –∞–ª—å–±–æ–º–æ–≤ –≤ Google Photos...\")\n",
    "albums = []\n",
    "nextPageToken = None\n",
    "\n",
    "while True:\n",
    "    response = service.albums().list(pageSize=50, pageToken=nextPageToken).execute()\n",
    "    albums.extend(response.get('albums', []))\n",
    "    nextPageToken = response.get('nextPageToken')\n",
    "    if not nextPageToken:\n",
    "        break\n",
    "\n",
    "target_albums = {album['title']: album['id'] for album in albums if album['title'] in ALBUM_NAMES}\n",
    "\n",
    "# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–ª—å–±–æ–º–æ–≤ ===\n",
    "if not target_albums:\n",
    "    print(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∞–ª—å–±–æ–º—ã –≤ Google Photos\")\n",
    "else:\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–ª—å–±–æ–º—ã: {list(target_albums.keys())}\")\n",
    "\n",
    "# === –ó–∞–ø–∏—Å—å –≤ CSV ===\n",
    "os.makedirs('csv', exist_ok=True)\n",
    "with open(OUTPUT_CSV, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['file_id', 'description'])\n",
    "\n",
    "    for album_name, album_id in target_albums.items():\n",
    "        print(f\"\\nüìÇ –ê–ª—å–±–æ–º: {album_name}\")\n",
    "        nextPageToken = None\n",
    "        \n",
    "        with tqdm(desc=f\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {album_name}\", unit=\"—Ñ–æ—Ç–æ\", leave=True) as pbar:\n",
    "            while True:\n",
    "                results = service.mediaItems().search(\n",
    "                    body={\"albumId\": album_id, \"pageSize\": 100, \"pageToken\": nextPageToken}\n",
    "                ).execute()\n",
    "\n",
    "                items = results.get('mediaItems', [])\n",
    "                nextPageToken = results.get('nextPageToken')\n",
    "\n",
    "                if not items:\n",
    "                    break\n",
    "\n",
    "                for item in items:\n",
    "                    file_id = item.get('id', 'No ID')\n",
    "                    description = item.get('description', 'No Description')\n",
    "                    writer.writerow([file_id, description])\n",
    "\n",
    "                # === –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ ===\n",
    "                pbar.update(len(items))\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "\n",
    "    print(f\"\\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441939e-2c18-4688-b1db-a6e001c01204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
